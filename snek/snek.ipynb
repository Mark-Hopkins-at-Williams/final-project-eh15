{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Train a Snake AI**\n",
    "\n",
    "![](./img/snek.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a guide on how to train an AI to play snake for you by using Deep Q Learning. There are three components: the snake game, the agent that plays the snake game, and the model.\n",
    "\n",
    "After the snake game has been made, we can start creating an agent and train it to play snake using reinforcement learning or deep q learning. In other words, we will reward the agent depending on how well it is doing and it will try to find the best next action based on the reward. Based on the game state, we calculate our next action based on the model prediction. We train our model using a feed-forward neural network with an input layer, hidden layer, and an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create your own snake game. Create a file for your game and import all necessary modules. Initialize pygame and create a class for the different directions (up, donw, left, right). For the speed, it is recommended to set it to a higher number in order to speed up training. The agent typically starts to improve after 80-100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "pygame.init()\n",
    "font = pygame.font.Font('arial.ttf', 25)\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "\n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "# rgb colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (200,0,0)\n",
    "GREEN1 = (0, 155, 0)\n",
    "GREEN2 = (0, 255, 0)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "SPEED = 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a class for the snake game. Initialize the display to your desired window size and create methods to reset the game state, randomly place food, to check if the game is over, and to reward the agent depending on if it has gotten food or if it collided/ended the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SnakeGameAI:\n",
    "    '''Initializing the game:'''\n",
    "    def __init__(self, w=240, h=240): #12x12 grid\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        # init display\n",
    "        self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        pygame.display.set_caption('Snake')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        # init game state\n",
    "        self.direction = Direction.RIGHT\n",
    "\n",
    "        self.head = Point(self.w/2, self.h/2)\n",
    "        self.snake = [self.head,\n",
    "                      Point(self.head.x-BLOCK_SIZE, self.head.y),\n",
    "                      Point(self.head.x-(2*BLOCK_SIZE), self.head.y)]\n",
    "\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        self.frame_iteration = 0\n",
    "\n",
    "    '''Randomly place food'''\n",
    "    def _place_food(self):\n",
    "        x = random.randint(0, (self.w-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        y = random.randint(0, (self.h-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:\n",
    "            self._place_food()\n",
    "\n",
    "    '''Check for collisions:'''\n",
    "    def is_collision(self, pt=None):\n",
    "        if pt is None:\n",
    "            pt = self.head\n",
    "        # hits boundary\n",
    "        if pt.x > self.w - BLOCK_SIZE or pt.x < 0 or pt.y > self.h - BLOCK_SIZE or pt.y < 0:\n",
    "            return True\n",
    "        # hits itself\n",
    "        if pt in self.snake[1:]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    '''Game rules and rewards:'''\n",
    "    def play_step(self, action):\n",
    "        self.frame_iteration += 1\n",
    "        # 1. collect user input\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "        \n",
    "        # 2. move\n",
    "        self._move(action) # update the head\n",
    "        self.snake.insert(0, self.head)\n",
    "        \n",
    "        # 3. check if game over\n",
    "        reward = 0\n",
    "        game_over = False\n",
    "        if self.is_collision() or self.frame_iteration > 100*len(self.snake):\n",
    "            game_over = True\n",
    "            reward = -10\n",
    "            return reward, game_over, self.score\n",
    "\n",
    "        # 4. place new food or just move\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "        \n",
    "        # 5. update ui and clock\n",
    "        self._update_ui()\n",
    "        self.clock.tick(SPEED)\n",
    "        # 6. return game over and score\n",
    "        return reward, game_over, self.score\n",
    "\n",
    "    def _update_ui(self):\n",
    "        self.display.fill(BLACK)\n",
    "\n",
    "        for pt in self.snake:\n",
    "            pygame.draw.rect(self.display, GREEN1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "            pygame.draw.rect(self.display, GREEN2, pygame.Rect(pt.x+4, pt.y+4, 12, 12))\n",
    "\n",
    "        pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "\n",
    "        text = font.render(\"Score: \" + str(self.score), True, WHITE)\n",
    "        self.display.blit(text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "        \n",
    "    '''Move the snake:'''\n",
    "    def _move(self, action):\n",
    "        # [straight, right, left]\n",
    "\n",
    "        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
    "        idx = clock_wise.index(self.direction)\n",
    "\n",
    "        if np.array_equal(action, [1, 0, 0]):\n",
    "            new_dir = clock_wise[idx] # no change\n",
    "        elif np.array_equal(action, [0, 1, 0]):\n",
    "            next_idx = (idx + 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # right turn r -> d -> l -> u\n",
    "        else: # [0, 0, 1]\n",
    "            next_idx = (idx - 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # left turn r -> u -> l -> d\n",
    "\n",
    "        self.direction = new_dir\n",
    "\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        if self.direction == Direction.RIGHT:\n",
    "            x += BLOCK_SIZE\n",
    "        elif self.direction == Direction.LEFT:\n",
    "            x -= BLOCK_SIZE\n",
    "        elif self.direction == Direction.DOWN:\n",
    "            y += BLOCK_SIZE\n",
    "        elif self.direction == Direction.UP:\n",
    "            y -= BLOCK_SIZE\n",
    "\n",
    "        self.head = Point(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the snake game has been made, we can start creating an agent and train it to play snake using reinforcement learning/deep Q learning. A brief explanation for how deep Q learning works is it is basically like trial and error, where the agent learns over time which decisions are better or worse depending on the rewards. For example, the agent is rewarded for eating the apple, so it will learn to go towards the apple. It is also punished (aka given a negative reward) for colliding into itself or the walls, so it will learn to avoid doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep Q learning, each Q value should improve the snake performance. We first initialize our model, then we either choose an action predicted as the best move based on observations from the game environment and prior knowledge, or a random move if we don't have enough information yet. \n",
    "\n",
    "A state is a representation of the environment observable by the agent. In this case, the state is a vector of 11 binary values (3 danger directions, 4 directions for the current direction the snake is going in, 4 food location directions). For the danger states, the states are set to 0 if there is no danger nearby (walls or snake body) but if it is near danger, the value turns 1). For the snake's direction, it can either be up, down, left, or right. The food location can be a combination of up, down, left, and/or right. \n",
    "\n",
    "There are 3 outputs for us to predict which the best action to take is (the reason why this is 3 instead of 4 is because the snake can't go behind itself; it can only go straight, left, or right)\n",
    "\n",
    "Deep Neural Network Architecture:\n",
    "\n",
    "![](./img/DNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the agent, we want to create these functions:\n",
    "- a function where we get the state of the game\n",
    "- a function where we remember the reward and calculate the next best action\n",
    "- a long term memory\n",
    "- short term member\n",
    "- get action based on the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from game import SnakeGameAI, Direction, Point\n",
    "from model import Linear_QNet, QTrainer\n",
    "from helper import plot\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "#can change these if you want to experiment:\n",
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001 #learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a function using all of the functions we made above that trains the agent. If the game is over, it resets the game, increments the game count, trains the agent with long term memory, and updates the record if necessary. It also keeps track of scores and mean scores for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    plot_scores = []\n",
    "    plot_mean_scores = []\n",
    "    total_score = 0\n",
    "    record = 0\n",
    "    agent = Agent()\n",
    "    game = SnakeGameAI()\n",
    "    while True:\n",
    "        # get curr state\n",
    "        state_curr = agent.get_state(game)\n",
    "\n",
    "        # get next move based on the state\n",
    "        next_move = agent.get_action(state_curr)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, game_over, score = game.play_step(next_move)\n",
    "        state_new = agent.get_state(game)\n",
    "\n",
    "        # train short memory\n",
    "        agent.train_short_memory(state_curr, next_move, reward, state_new, game_over)\n",
    "\n",
    "        # remember\n",
    "        agent.remember(state_curr, next_move, reward, state_new, game_over)\n",
    "\n",
    "        if game_over:\n",
    "            # train long memory aka replay memory or experience memory\n",
    "            game.reset()\n",
    "            agent.n_games += 1\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
    "            \n",
    "            #plotting\n",
    "            plot_scores.append(score) #append score to plot list\n",
    "            total_score += score #update total score\n",
    "            mean_score = total_score / agent.n_games\n",
    "            plot_mean_scores.append(mean_score) #append mean score to plot list\n",
    "            plot(plot_scores, plot_mean_scores) #plot scores and mean scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run the program, we will need to create a class for the model and a class for the trainer.\n",
    "\n",
    "The model uses a simple forward function that:\n",
    "    \n",
    "    1. Applies first linear layer (self.linear1) to the input tensor x.\n",
    "    \n",
    "    2. Applies activation function to the output of the first linear layer.\n",
    "    \n",
    "    3. Applies second linear layer (self.linear2) to the output of the activation function.\n",
    "    \n",
    "    4. Returns the final output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.linear1(x)) #also good performer\n",
    "        #x = F.gelu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear1(x)) #best performer\n",
    "        #x = torch.tanh(self.linear1(x))\n",
    "        #x = F.sigmoid(self.linear1(x))\n",
    "        #x = F.softmax(self.linear1(x), dim = 0)\n",
    "    \n",
    "        #x = F.elu(self.linear1(x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "    #saves the model's state dictionary\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our trainer, we use the Bellman equation, which can be simplified to \n",
    "\n",
    "![](./img/trainer.png)\n",
    "\n",
    "What this basically means is if the game is over, our target value Q new is set to equal the immediate reward before the game ended. Else our new Q value is set to the the immediate reward plus the discounted maximum predicted Q value for the next state (aka discount factor (gamma) * predicted Q value of the next state)\n",
    "\n",
    "We will also use the loss function:\n",
    "\n",
    "![](./img/loss.png)\n",
    "\n",
    "Which is basically the mean squared error between the target Q values and the predicted Q values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr) #optimization algo\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, gameover):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "\n",
    "        #unsqueeze to match dim\n",
    "        if len(state.shape) == 1:\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            gameover = (gameover, )\n",
    "\n",
    "        # 1: predicted Q values with current state\n",
    "        predQ = self.model(state)\n",
    "\n",
    "        targetQ = predQ.clone()\n",
    "        for idx in range(len(gameover)):\n",
    "            Q_new = reward[idx]\n",
    "            if not gameover[idx]:\n",
    "                # Bellman equation:\n",
    "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
    "\n",
    "            targetQ[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "\n",
    "        # Apply the loss function:\n",
    "        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if game not over\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.MSELoss(targetQ, predQ)\n",
    "        loss.backward() #back propagation to compute gradients\n",
    "\n",
    "        self.optimizer.step() #update model parameters based on the computed gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra potential experiments to improve performance:\n",
    "- changing the reward values (i.e. giving higher reward for eating apple or higher negative reward for colliding into something)\n",
    "- testing different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested out more activation functions. A variety of built-in activation functions can be found in torch.nn already. Let's test out GELU first:\n",
    "\n",
    "![](./img/geluform.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gelu: (manual implementation below, but this is also in torch.nn)\n",
    "import math\n",
    "def gelu(x):\n",
    "  return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it looks like GELU is not great at snake game:\n",
    "\n",
    "![](./img/GELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same goes for many other activation functions other than RELU and LeakyRELU:\n",
    "\n",
    "Sigmoid:\n",
    "\n",
    "![](./img/sigmoid.png)\n",
    "\n",
    "Softmax:\n",
    "\n",
    "![](./img/softmax.png)\n",
    "\n",
    "Tanh:\n",
    "\n",
    "![](./img/tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RELU:\n",
    "\n",
    "![](./img/RELU.png)\n",
    "\n",
    "LeakyRELU:\n",
    "![](./img/LeakyRELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers referenced:\n",
    "\n",
    "A. Sebastianelli, M. Tipaldi, S. L. Ullo and L. Glielmo, \"A Deep Q-Learning based approach applied to the Snake game,\" 2021 29th Mediterranean Conference on Control and Automation (MED), PUGLIA, Italy, 2021, pp. 348-353, doi: 10.1109/MED51440.2021.9480232.\n",
    "\n",
    "Hendrycks, Dan, and Kevin Gimpel. \"Gaussian error linear units (gelus).\" arXiv preprint arXiv:1606.08415 (2016)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0c820d97483d5354bd04ffc31e4d575c39a6cb056049fa0889c57fa2f7cc600"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
